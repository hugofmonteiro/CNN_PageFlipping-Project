{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ef73e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 00:06:37.845991: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/hugo/tensorflow-metal-test/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d3c3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing --- Goal to make the data compatible for CNN input\n",
    "# In tensorflow, we can achieve the same using ImageGenerators\n",
    "\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale= 1.0/255.)\n",
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale= 1.0/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "637a4d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2392 images belonging to 2 classes.\n",
      "Found 597 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Pass the images through the generator\n",
    "trainImageData = train_generator.flow_from_directory(\"../data/training\",\n",
    "                                                     batch_size=32, #how much images to give per iteration in an epoch\n",
    "                                                     class_mode=\"binary\", #Incase of multi-class classification, \"categorical\"\n",
    "                                                     target_size=(64,64) #Ensures all images are of same size (resizing)\n",
    "                                                     ) \n",
    "\n",
    "\n",
    "testImageData = train_generator.flow_from_directory(\"../data/testing\",\n",
    "                                                     batch_size=32, #how much images to give per iteration in an epoch\n",
    "                                                     class_mode=\"binary\", #Incase of multi-class classification, \"categorical\"\n",
    "                                                     target_size=(64,64) #Ensures all images are of same size (resizing)\n",
    "                                                     ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dee1302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flip': 0, 'notflip': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Given dataset is a balanced dataset\n",
    "\n",
    "trainImageData.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f8bf3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainImageData.image_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d17f3",
   "metadata": {},
   "source": [
    "# Transferlearn from VGG16 - keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6fb4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df94df8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing the VGG16 model but discarding the last 1000 neuron layer for adjustment to our dataset\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92b98451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the base_model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "415f495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Single neuron for binary classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7f88a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 2, 2, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              525312    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,241,025\n",
      "Trainable params: 526,337\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a49c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\", #For multi-class classification: categorical_crossentropy | sparse_categorical_crossentropy\n",
    "              metrics=[tfa.metrics.F1Score(num_classes=1, threshold=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf1ce6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainImageData.filenames) // trainImageData.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a468b030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 00:12:16.939401: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-01-29 00:12:17.410474: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-01-29 00:12:17.445905: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp_2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - ETA: 0s - loss: 0.5361 - f1_score: 0.7342"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 00:12:48.860565: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-01-29 00:12:48.942856: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 39s 500ms/step - loss: 0.5361 - f1_score: 0.7342 - val_loss: 0.5012 - val_f1_score: 0.8017\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 37s 505ms/step - loss: 0.3447 - f1_score: 0.8739 - val_loss: 0.3158 - val_f1_score: 0.8818\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 37s 507ms/step - loss: 0.2518 - f1_score: 0.9111 - val_loss: 0.2417 - val_f1_score: 0.9198\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 37s 508ms/step - loss: 0.2038 - f1_score: 0.9309 - val_loss: 0.2152 - val_f1_score: 0.9220\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 37s 507ms/step - loss: 0.1763 - f1_score: 0.9377 - val_loss: 0.2409 - val_f1_score: 0.9188\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 38s 508ms/step - loss: 0.1764 - f1_score: 0.9307 - val_loss: 0.1810 - val_f1_score: 0.9418\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 38s 508ms/step - loss: 0.1391 - f1_score: 0.9517 - val_loss: 0.2011 - val_f1_score: 0.9171\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 38s 510ms/step - loss: 0.1182 - f1_score: 0.9629 - val_loss: 0.1634 - val_f1_score: 0.9468\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 37s 507ms/step - loss: 0.1151 - f1_score: 0.9618 - val_loss: 0.1545 - val_f1_score: 0.9525\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 37s 506ms/step - loss: 0.1071 - f1_score: 0.9649 - val_loss: 0.1484 - val_f1_score: 0.9571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d9a90850>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(trainImageData,\n",
    "          validation_data=testImageData,\n",
    "          epochs=10,\n",
    "          steps_per_epoch = len(trainImageData.filenames) // trainImageData.batch_size,\n",
    "          validation_steps= len(testImageData.filenames) // testImageData.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9593399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/VGG16.keras')\n",
    "#model.save('monReader.keras_tf', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c923b",
   "metadata": {},
   "source": [
    "### The VGG16 re-trained to our dataset is 65MB, so it will NOT fit in a smartphone app! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-metal-test",
   "language": "python",
   "name": "tensorflow-metal-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
